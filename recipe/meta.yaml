{% set mxnet_version="1.9.1" %}
{% set hash_value="cef85932e2b3caead235008473d29512b99581c07da3d10703ff5b6c1fb5bd50" %}
{% set build_number="0" %}


{% set dlpack_git_hash = "3efc489b55385936531a06ff83425b719387ec63" %}
{% set dlpack_sha_hash = "b59586ce69bcf3efdbf3cf4803fadfeaae4948044e2b8d89cf912194cf28f233" %}

{% set dmlccore_git_hash = "5df8305fe699d3b503d10c60a231ab0223142407" %}
{% set dmlccore_sha_hash = "a8046f752f36005564d2924b4b6f73e1aea3cce7ff10f9e19d99ad6a22a045b2" %}

{% set googletest_git_hash = "eb9225ce361affe561592e0912320b9db84985d0" %}
{% set googletest_sha_hash = "a4cb4b0c3ebb191b798594aca674ad47eee255dcb4c26885cf7f49777703484f" %}

{% set intgemm_git_hash = "8f28282c3bd854922da638024d2659be52e892e9" %}
{% set intgemm_sha_hash = "bc8bd8015613a13747eb769876385ec53e8c1ea7ae3f8414521dc53b8fcdfc65" %}

{% set nvidia_cub_git_hash = "0158fa19f28619886232defd412433974af89611" %}
{% set nvidia_cub_sha_hash = "43424c4c17a997d1d730c89ec14688671245de7941e02b388d7d3df6ea53777a" %}

# This are future/2.x variables.
#{% set onednn_git_hash = "58be3660fb57c4c4a3d306730e849237d1271572" %}
#{% set onednn_sha_hash = "0d170c30ea0a35fbe48008e0e7e729d0993720c14f320ce8e73a891ae176eec4" %}

{% set onnxtensorrt_git_hash = "2eb74d933f89e1590fdbfc64971a36e5f72df720" %}
{% set onnxtensorrt_sha_hash = "df99819727445c247fb5c21c2fd825ded3269376867457ae84fa6d6f1c0ae331" %}

{% set pslite_git_hash = "34fd45cae457d59850fdcb2066467778d0673f21" %}
{% set pslite_sha_hash = "ec5d5baab8bbf0c3983ad5f18d7f963f15ae7cd4d154ec204b03c1dceccf209b" %}

{% set tvm_git_hash = "9bd2c7b44208ed992061f8c2688e1137357f1db1" %}
{% set tvm_sha_hash = "68d0c2f14bd00db2dc90fc1a2ed389a3d04d9176c5f96133dfa44b81c28cdd89" %}


package:
  name: mxnet-suite
  version: {{ mxnet_version }}

source:
  - url:  https://github.com/apache/incubator-mxnet/archive/refs/tags/{{ mxnet_version }}.tar.gz
    sha256: {{ hash_value }}
    patches:
      - patches/0001-cpp-std-14.patch
      - patches/0001-make-graphviz-optional.patch
      - patches/0002-conda-unbundle-libmxnet-DSO.patch

  - url: https://github.com/dmlc/dlpack/archive/{{ dlpack_git_hash }}.tar.gz
    fn: dlpack_{{ dlpack_git_hash }}.tar.gz
    sha256: {{ dlpack_sha_hash }}
    folder: 3rdparty/dlpack
    patches:
      - patches/0011-cpp-std-14.patch

  - url: https://github.com/dmlc/dmlc-core/archive/{{ dmlccore_git_hash }}.tar.gz
    fn: dmlc-core_{{ dmlccore_git_hash }}.tar.gz
    sha256: {{ dmlccore_sha_hash }}
    folder: 3rdparty/dmlc-core

  - url: https://github.com/google/googletest/archive/{{ googletest_git_hash }}.tar.gz
    fn: googletest_{{ googletest_git_hash }}.tar.gz
    sha256: {{ googletest_sha_hash }}
    folder: 3rdparty/googletest

  - url: https://github.com/kpu/intgemm/archive/{{ intgemm_git_hash }}.tar.gz
    fn: intgemm_{{ intgemm_git_hash }}.tar.gz
    sha256: {{ intgemm_sha_hash }}
    folder: 3rdparty/intgemm
    patches:
      - patches/0041-cpp-std-14.patch

  - url: https://github.com/NVlabs/cub/archive/{{ nvidia_cub_git_hash }}.tar.gz
    fn: nvidia_cub_{{ nvidia_cub_git_hash }}.tar.gz
    sha256: {{ nvidia_cub_sha_hash }}
    folder: 3rdparty/nvidia_cub

  # This will be useful for 2.x builds
  #  - url: https://github.com/oneapi-src/oneDNN/archive/{{ onednn_git_hash }}.tar.gz
  #    fn: onednn_{{ onednn_git_hash }}.tar.gz
  #    sha256: {{ onednn_sha_hash }}
  #    folder: 3rdparty/onednn

  - url: https://github.com/onnx/onnx-tensorrt/archive/{{ onnxtensorrt_git_hash }}.tar.gz
    fn: onnx-tensorrt_{{ onnxtensorrt_git_hash }}.tar.gz
    sha256: {{ onnxtensorrt_sha_hash }}
    folder: 3rdparty/onnx-tensorrt

  - url: https://github.com/dmlc/ps-lite/archive/{{ pslite_git_hash }}.tar.gz
    fn: ps-lite_{{ pslite_git_hash }}.tar.gz
    sha256: {{ pslite_sha_hash }}
    folder: 3rdparty/ps-lite

  - url: https://github.com/apache/incubator-tvm/archive/{{ tvm_git_hash }}.tar.gz
    fn: tvm_{{ tvm_git_hash }}.tar.gz
    sha256: {{ tvm_sha_hash }}
    folder: 3rdparty/tvm


# Right now there is no valid win-64 configuration (no mkl). So skip it. We are mxnet_blas_impl as a workaround for conda.
build:        # [mxnet_blas_impl == 'invalid']
  skip: true  # [mxnet_blas_impl == 'invalid']

requirements:
  build:
    - patch     # [not win]
    - m2-patch  # [win]


outputs:

  - name: _mutex_mxnet
    version: {{ mxnet_variant_version }}
    script: build-nothing.bat  # [win]
    script: build-nothing.sh   # [not win]

    build:
      string: {{ mxnet_variant_str }}
      number: {{ build_number }}

    about:
      home: https://mxnet.apache.org
      summary: Mutex package to pin a variant of MXNet Conda package
      dev_url: https://github.com/apache/incubator-mxnet
      doc_url: https://mxnet.incubator.apache.org/

  - name: libmxnet
    version: {{ mxnet_version }}
    script: build-libmxnet.bat  # [win]
    script: build-libmxnet.sh   # [not win]

    build:
      string: {{ mxnet_variant_str }}_h{{PKG_HASH}}_{{ build_number }}
      number: {{ build_number }}
      skip: True              # [win and vc<14]
      missing_dso_whitelist:  # [linux]
        - libcuda.so.1        # [linux]
        - '$RPATH/ld64.so.1'  # [s390x] This is missing from the activation script.
        - '**libmxnet.dylib'  # [osx]  This is a quirk in conda-build's testing
    requirements:
      build:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - pkg-config  # [not win]
        - ninja-base
        - cmake
        - python
      host:
        #    - jemalloc                               # [linux]
        - mkl-dnn                                # [unix and (mxnet_blas_impl == 'mkl')]
        - mklml                                  # [unix and (mxnet_blas_impl == 'mkl')]
        - mkl-devel                              # [mxnet_blas_impl == 'mkl']
        - intel-openmp                           # [mxnet_blas_impl == 'mkl']
        - openblas-devel                         # [mxnet_blas_impl == 'openblas']
        - libopencv >=4.6                        # [not s390x]
        - imagecodecs                            # [win]
        - cudatoolkit {{ cudatoolkit_version }}  # ['gpu' in str(mxnet_variant_str)]
        - cudnn {{ cudnn_version }}              # ['gpu' in str(mxnet_variant_str)]
      run:
        - _mutex_mxnet {{ mxnet_variant_version }} {{ mxnet_variant_str }}
        - cudatoolkit {{ cudatoolkit_version }}  # ['gpu' in str(mxnet_variant_str)]
        - cudnn {{ cudnn_version }}              # ['gpu' in str(mxnet_variant_str)]
        - {{ pin_compatible('mkl-dnn') }}        # [unix and (mxnet_blas_impl == 'mkl')]
        - {{ pin_compatible('intel-openmp') }}   # [mxnet_blas_impl == 'mkl']
        - _openmp_mutex                          # [linux]
        - opencv >=4.6                           # [not s390x]

    test:
      commands:
        - im2rec -h   # [not (win or s390x)]
      requires:       # [x86 and mxnet_blas_impl != 'mkl']
        - nomkl       # [x86 and mxnet_blas_impl != 'mkl']

    about:
      home: https://mxnet.apache.org
      license: Apache-2
      license_file: LICENSE
      license_family: Apache
      summary: MXNet is a deep learning framework designed for both efficiency and flexibility
      description: |
        Apache MXNet (incubating) is a deep learning framework designed for both
        efficiency and flexibility. It allows you to mix symbolic and imperative
        programming to maximize efficiency and productivity. At its core, MXNet
        contains a dynamic dependency scheduler that automatically parallelizes both
        symbolic and imperative operations on the fly. A graph optimization layer on
        top of that makes symbolic execution fast and memory efficient. MXNet is
        portable and lightweight, scaling effectively to multiple GPUs and multiple
        machines. MXNet is also more than a deep learning project. It is also a
        collection of blue prints and guidelines for building deep learning systems,
        and interesting insights of DL systems for hackers.
      dev_url: https://github.com/apache/incubator-mxnet
      doc_url: https://mxnet.incubator.apache.org/


  - name: py-mxnet
    version: {{ mxnet_version }}
    script: build-py-mxnet.bat  # [win]
    script: build-py-mxnet.sh   # [not win]

    build:
      number: {{ build_number }}

    requirements:
      build:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
      host:
        - {{ pin_subpackage('libmxnet', exact=True) }}
        - nomkl                                # [mxnet_blas_impl == 'openblas']
        - cython
        - python
        # Numpy's C API is not used
        - numpy
        - requests >=2.20.0,<3
        - setuptools
      run:
        - {{ pin_subpackage('libmxnet', exact=True) }}
        - numpy >1.16.0,<=2.0.0
        - requests >=2.20.0,<3
        - python

    test:
      script: py-mxnet_test.py
      imports:
        - mxnet

    about:
      home: https://mxnet.apache.org
      license: Apache-2
      license_file: LICENSE
      license_family: Apache
      summary: MXNet is a deep learning framework designed for both efficiency and flexibility
      description: |
        Apache MXNet (incubating) is a deep learning framework designed for both
        efficiency and flexibility. It allows you to mix symbolic and imperative
        programming to maximize efficiency and productivity. At its core, MXNet
        contains a dynamic dependency scheduler that automatically parallelizes both
        symbolic and imperative operations on the fly. A graph optimization layer on
        top of that makes symbolic execution fast and memory efficient. MXNet is
        portable and lightweight, scaling effectively to multiple GPUs and multiple
        machines. MXNet is also more than a deep learning project. It is also a
        collection of blue prints and guidelines for building deep learning systems,
        and interesting insights of DL systems for hackers.
      dev_url: https://github.com/apache/incubator-mxnet
      doc_url: https://mxnet.incubator.apache.org/


  - name: mxnet
    version: {{ mxnet_version }}
    script: build-nothing.bat  # [win]
    script: build-nothing.sh   # [not win]

    build:
      number: {{ build_number }}

    requirements:
      run:
        - {{ pin_subpackage('libmxnet', exact=True) }}
        - {{ pin_subpackage('py-mxnet', exact=True) }}

    about:
      home: https://mxnet.apache.org
      summary: MXNet metapackage for installing lib,py-MXNet Conda packages
      dev_url: https://github.com/apache/incubator-mxnet
      doc_url: https://mxnet.incubator.apache.org/

  - name: mxnet-{{ mxnet_variant_str }}
    version: {{ mxnet_version }}
    script: build-nothing.bat  # [win]
    script: build-nothing.sh   # [not win]

    #build:
      #number: {{ build_number }}

    requirements:
      run:
        - {{ pin_subpackage('libmxnet', exact=True) }}
        - {{ pin_subpackage('_mutex_mxnet', exact=True) }}

    about:
      home: https://mxnet.apache.org
      summary: MXNet metapackage which pins a variant of MXNet Conda package
      dev_url: https://github.com/apache/incubator-mxnet
      doc_url: https://mxnet.incubator.apache.org/

  - name: mxnet-gpu
    version: {{ mxnet_version }}
    script: build-nothing.bat  # [win]
    script: build-nothing.sh   # [not win]

    build:
      #number: {{ build_number }}
      # Skip when the variant is NOT a gpu build.
      skip: true                             # [not ('gpu' in str(mxnet_variant_str))]

    requirements:
      run:
        - {{ pin_subpackage('libmxnet', exact=True) }}
        - {{ pin_subpackage('_mutex_mxnet', exact=True) }}

    about:
      home: https://mxnet.apache.org
      summary: MXNet metapackage which pins a variant of MXNet(GPU) Conda package
      dev_url: https://github.com/apache/incubator-mxnet
      doc_url: https://mxnet.incubator.apache.org/

extra:
  recipe-maintainers:
    - jjhelmus
    - nehaljwani
